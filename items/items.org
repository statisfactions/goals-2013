#+TITLE: Goals 2013 Item analysis
#+OPTIONS: toc:nil
#+AUTHOR: Ethan Brown
#+LaTex_HEADER: \usepackage[cm]{fullpage}
#+LaTex_HEADER: \pagestyle{empty}
#+LaTex_HEADER: \thispagestyle{empty}
#+LaTex_HEADER: \DeclareUnicodeCharacter{00A0}{~}

#+BEGIN_SRC R :ravel setup
  source("/home/fortis/goals-2013/descriptives/descriptives.R")
  library(psych)
#+END_SRC

* Basic item analysis

#+BEGIN_SRC R :ravel alpha
  al <- alpha(gp.rw)
  al
#+END_SRC

Coeff alpha is at 0.67, seems a bit low for a test like this.  The within-subject variance is higher than would be expected if the items were all really measuring the same thing.  I believe 0.7 is the usual cutoff for studies like this?

I'm not sure how surprising this is, since I don't necessarily expect that ``statistical reasoning'' is a unitary concept.  There's very clearly several subscales and distinct concepts that we would expect to go different directions, such as confidence intervals, study design, /p/-values, etc.

I don't see any cause for concern for the changes in reliability here.

#+BEGIN_SRC R :ravel cttDiscrim,fig=TRUE
  r.cor <- al$item.stats$r.cor
  names(r.cor) <- 1:23
  barplot(rev(r.cor), horiz = T, las = 2, main = "Item discrimination (r.cor)")
  abline(v=0.2, col = "red")
  
#+END_SRC

Both of the herbicide items, including the supposed sample size one (q06) had very poor discrimination.  There may be some confusion about what exactly this item is saying or implying. The worst discrimination was on q16, the confidence interval/prediction for a single case (which was a relatively difficult item, as well, PC = $~ 0.34$).  q09, the candies problem, had decent discimination, q23 had acceptable, and q13 had excellent discrimination.

* Distractor analysis

** Herbicide (q06)
#+BEGIN_SRC R :ravel distractors.setup
  source("/home/fortis/goals-2013/functions.R")
  
  d06 <- distractors("q06", gp)
  lab06 <- "Sample size may be why not significant"
  d09 <- distractors("q09", gp)
  lab09 <- "Variability difference"
  d13 <- distractors("q13", gp)
  lab13 <- "CI width difference"
  d23 <- distractors("q23", gp)
  lab23 <- "p-value comparison"
  
  
#+END_SRC

#+BEGIN_SRC R :ravel distract06,fig=TRUE,width=9
  plq06 <- ggplot(d06, aes(pct.minus, scalecount, color = col)) +
      geom_line() +
      theme_bw() +
      xlab("Proportion correct (excluding this item)") +
      ylab("Students") +
      ggtitle("Herbicide/sample size (q06)") +
      coord_cartesian(ylim = c(0, 1)) + 
      scale_color_discrete(name = lab06) +
      theme(legend.position = "top")
  print(plq06)
#+END_SRC

#+BEGIN_SRC R
  round(prop.table(table(gp$q06)), 2)
#+END_SRC

The Herbicide sample size item appears to be telling us very little about differences between students.  Across ability levels, 75\% of students got the item correct pretty stably, as indicated by the relatively flat line above (only really diverging at around 0.9 PC, which is a tiny number of students).

This may suggest a common intuitive reaction to this item that is independent of the other items in the instrument.

** Candies (q09)

#+BEGIN_SRC R :ravel distract09,fig=TRUE,width=9
  ## name conflict with %+%, which is why I need to use colon notation here
  
  plq09 <- ggplot2::"%+%"(plq06, d09) + 
      scale_color_discrete(name = lab09) +
      ggtitle("Candies problem (q09)")
  plq09
    
#+END_SRC

#+BEGIN_SRC R
  round(prop.table(table(gp$q09)), 2)
#+END_SRC
Clearly, the dominant misconception here is thinking that the two distributions of sample statistics are essentially the same (0.34).  This answer is neck in neck with the correct answers for the modal students (btw around .25 and .45 PC), where it starts to diverge as proportion correct increases.  Unsurprisingly this remains a strong misconception.

** CI/Sample size (q13)
#+BEGIN_SRC R :ravel distract13,fig=TRUE,width=9
  ## name conflict with %+%, which is why I need to use colon notation here
  
  plq13 <- ggplot2::"%+%"(plq06, d13) +
      scale_color_discrete(name = lab13) +
      ggtitle("CIs and sample size (q13)")
  plq13
    
#+END_SRC

#+BEGIN_SRC R
  round(prop.table(table(gp$q13)), 2)
#+END_SRC

Interestingly, the prominent misconception on confidence intervals is different: people are more likely to think that larger CIS are wider, which may represent some basic misunderstandings of the way that confidence intervals work and what they represent.

Based on the ``size-confidence intuition'', I was expecting this item to be easier than the candies problem, but that of course presupposes that people have a solid understanding of confidence intervals and what they represent (which is certainly not a given!).  Since this is formally so similar to the candies problem, it is a good question why a different distractor seems to dominate here.

This is perhaps the most puzzling of the results, because the usual misconception is what I would term sample size insensitivity, which would favor the ``same'' response.  Something else is afoot here, though it may be localized to familiar misconceptions about confidence intervals.

*** TODO Lit search on CI misconceptions?

** P-value and sample size (q23)

#+BEGIN_SRC R :ravel distract23,fig=TRUE,width=9
  ## name conflict with %+%, which is why I need to use colon notation here
  
  plq23 <- ggplot2::"%+%"(plq06, d23) + 
      scale_color_discrete(name = lab23) +
      ggtitle("P-value and sample size (q23)")
  plq23
    
#+END_SRC
#+BEGIN_SRC R
  round(prop.table(table(gp$q23)), 2)
#+END_SRC

Though somewhat more difficult, this item appears more in line with the results and the expected misconceptions from the candies problem and is a very similar graph.. Again, the /p/-value is a complex compound construct, and the formal equivalency may be masking other differences in the tasks.  We'll perhaps see this more clearly once we do our mosaic plots.

* Distractor mosaics
