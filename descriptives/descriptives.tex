% Created 2013-12-27 Fri 03:06
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage{wasysym}
\usepackage{amssymb}
\usepackage{hyperref}
\tolerance=1000
\usepackage[cm]{fullpage}
\pagestyle{empty}
\thispagestyle{empty}
\DeclareUnicodeCharacter{00A0}{~}
\author{Ethan Brown}
\date{\today}
\title{Goals 2013 Descriptives}
\hypersetup{
  pdfkeywords={},
  pdfsubject={},
  pdfcreator={Emacs 24.2.1 (Org mode 8.2.1)}}
\usepackage{Sweave}
\begin{document}

\maketitle

\section{Setup}
\label{sec-1}
\begin{Schunk}
\begin{Sinput}
> library(car)
> gp.raw <- read.csv("/home/fortis/goals-2013/data-in/GOALS-DEIDENTIFIED-2013-12-20.csv")
> ## Exclude non-consenting
> summary(factor(gp.raw$consent)) ## no missing values; 38 nonconsenting, 1165 consenting
\end{Sinput}
\begin{Soutput}
   1    2 
1165   38 
\end{Soutput}
\begin{Sinput}
> gp <- gp.raw[ gp.raw$consent == 1, ] ## 2 = non-consenting
> ## Replace response numeric codes with factors for reasonable summarizing
> responsecols <- grep("^q..$", names(gp), value = T)
> gp[,responsecols] <- lapply(gp[,responsecols], factor)
> rwcols <- paste0(responsecols, ".rw")
> gp.rw <- gp[, rwcols]
\end{Sinput}
\end{Schunk}

\begin{Schunk}
\begin{Sinput}
> ## Add response options
> 
> ## sampling items
> gp$q06 <- factor(gp$q06, labels = c("Valid", "Invalid"))
> gp$q09 <- factor(gp$q09, labels = c("Same", "Small n is wider", "Large n is wider"))
> gp$q13 <- factor(gp$q13, labels = c("Small n is wider", "Large n is wider", "Same"))
> gp$q23 <- factor(gp$q23, labels = c("Smaller than original", "Same as original", "Larger than original"))
> ## center items
> gp$q10 <- factor(gp$q10, labels = c("Standard medication", "New medication", "Neither"))
> gp$q11 <- factor(gp$q11, labels = c("Smaller difference", "Larger difference", "Same evidence"))
> ## Variability items
> 
> gp$q08 <- factor(gp$q08, labels = c("Uniform", "Unimodal", "Neither"))
> gp$q12 <- factor(gp$q12, labels = c("Smaller variability", "Larger variability", "Same evidence"))
\end{Sinput}
\end{Schunk}

\section{Missing values}
\label{sec-2}
\begin{Schunk}
\begin{Sinput}
> table(gp$missing)
\end{Sinput}
\begin{Soutput}
   0    1    2    3    4 
1118   41    4    1    1 
\end{Soutput}
\end{Schunk}

\section{Items}
\label{sec-3}

\subsection{All Items}
\label{sec-3-1}
\begin{center}
\begin{tabular}{rl}
\textbf{Items} & \textbf{Measured Learning Goal}\\
\hline
1 & Able to reason about the purpose of random assignment\\
2 & Able to reason about the factors that allow a sample of data to be representative of the population.\\
3 & Able to reason about how correlation does not imply causation.\\
4 & Able to reason about the effect of  moving an influential point to a new location on the correlation coefficient\\
5 & Able to reason that no statistical significance does not guarantee that there is no effect.\\
6 & Able to reason about the relationship between sample size and statistical significance\\
7 & Able to reason about factors that affect the mean and median\\
8 & Able to reason that given two distributions that have the same range, the one with less mass in the center has the larger standard deviation\\
9 & Able to reason about differences between empirical sampling distributions for large and small sample sizes.\\
10 & Able to reason about how to judge differences when visually comparing two samples of data.\\
11 & Able to reason about how differences in center affect strength of evidence against the null hypothesis of no difference\\
12 & Able to reason about how differences in variability affect strength of evidence against the null hypothesis of no difference\\
13 & Able to reason about how the width of a confidence interval is related to sample size.\\
14 & Able to reason about how the width of a confidence interval is related to level of confidence.\\
15 & Able to reason about a misinterpretation of a confidence level (the range of observed values for individual observational units or cases)\\
16 & Able to reason about a misinterpretation of a confidence level (using it to make a prediction for a single case)\\
17 & Able to reason about what the null model represents in a research study\\
18 & Able to reason about what model should be used for the null hypothesis when comparing two groups\\
19 & Able to reason about how to find a p-value based on simulation results.\\
20 & Able to reason about a conclusion based on a statistically significant p-value in the context of a research study that compares two groups\\
21 & Able to reason about an incorrect interpretation of a p-value (probability of a treatment being more effective).\\
22 & Able to reason about the correct interpretation of a p-value.\\
23 & Able to reason about how increasing the sample size affects the p-value, all else being equal.\\
\end{tabular}
\end{center}

\begin{Schunk}
\begin{Sinput}
> library(corrgram)
> corrgram(gp.rw, cex.labels = 1.5, lower.panel = panel.shade, upper.panel = panel.pie)
\end{Sinput}
\end{Schunk}
\includegraphics{descriptives-corrgram}

Bivariate correlations seem generally quite low.  The mean/median item (q07) has among the largest number of substantially correlated items.  q09, the candies item, and q13, the CI/Sample size item, both have a lot of healthy intercorrelations.

The Alice testlet seems to pop out quite separately from the other items\ldots{} we may be getting a very different group of students who do well on the Alice items than the others (but this conclusion should be taken with caution as it's based only on bivariate correlations).  It would be interesting to do some principle components on this.
fdjksalf
\begin{Schunk}
\begin{Sinput}
> library(ggplot2)
> pc <- colMeans(gp.rw, na.rm = T)
> round(pc, 2)
\end{Sinput}
\begin{Soutput}
q01.rw q02.rw q03.rw q04.rw q05.rw q06.rw q07.rw q08.rw q09.rw q10.rw q11.rw 
  0.27   0.64   0.17   0.42   0.54   0.73   0.68   0.43   0.50   0.74   0.76 
q12.rw q13.rw q14.rw q15.rw q16.rw q17.rw q18.rw q19.rw q20.rw q21.rw q22.rw 
  0.65   0.47   0.63   0.59   0.34   0.41   0.35   0.34   0.40   0.49   0.64 
q23.rw 
  0.45 
\end{Soutput}
\begin{Sinput}
> names(pc) <- 1:23
> barplot(pc, las = 2)
\end{Sinput}
\end{Schunk}
\includegraphics{descriptives-percentCorrect}



\subsubsection{{\bfseries\sffamily TODO} replace this table with glosses/descriptions of the items for quick reference}
\label{sec-3-1-1}

\subsection{Sample size}
\label{sec-3-2}
\begin{center}
\begin{tabular}{rl}
\textbf{Items} & \textbf{Gloss}\\
\hline
6 & Herbicide: sample size may be why not significant\\
9 & Candies problem (visual)\\
13 & CI/Sample size\\
23 & Increase in sample size -> p-value\\
\end{tabular}
\end{center}

\begin{Schunk}
\begin{Sinput}
> samps <- c("q06", "q09", "q13", "q23")
> samps.rw <- paste0(samps, ".rw")
> summary(gp[,sort(c(samps, samps.rw))])
\end{Sinput}
\begin{Soutput}
      q06          q06.rw                     q09          q09.rw   
 Valid  :844   Min.   :0.0000   Same            :400   Min.   :0.0  
 Invalid:320   1st Qu.:0.0000   Small n is wider:581   1st Qu.:0.0  
 NA's   :  1   Median :1.0000   Large n is wider:181   Median :0.5  
               Mean   :0.7251   NA's            :  3   Mean   :0.5  
               3rd Qu.:1.0000                          3rd Qu.:1.0  
               Max.   :1.0000                          Max.   :1.0  
               NA's   :1                               NA's   :3    
               q13          q13.rw                          q23     
 Small n is wider:548   Min.   :0.0000   Smaller than original:520  
 Large n is wider:396   1st Qu.:0.0000   Same as original     :395  
 Same            :210   Median :0.0000   Larger than original :246  
 NA's            : 11   Mean   :0.4749   NA's                 :  4  
                        3rd Qu.:1.0000                              
                        Max.   :1.0000                              
                        NA's   :11                                  
     q23.rw      
 Min.   :0.0000  
 1st Qu.:0.0000  
 Median :0.0000  
 Mean   :0.4479  
 3rd Qu.:1.0000  
 Max.   :1.0000  
 NA's   :4       
\end{Soutput}
\begin{Sinput}
> round(cor(gp[,samps.rw], use = "complete.obs"), 2)
\end{Sinput}
\begin{Soutput}
       q06.rw q09.rw q13.rw q23.rw
q06.rw   1.00  -0.04   0.02   0.04
q09.rw  -0.04   1.00   0.28   0.13
q13.rw   0.02   0.28   1.00   0.24
q23.rw   0.04   0.13   0.24   1.00
\end{Soutput}
\end{Schunk}

As noted above in ``All Items'', both 9 and 13 are items that seem to have a lot in common with several other items, although I haven't seen whether those items are all visual items (both 9 and 13 are visual).

q06, the herbicide item, was one of the easier items, with 72\% correct.  The item presents subjects with the possibility that too small sample size, even in the presence of an effect, may account for the finding.

Formally, this is rather similar to q23, also an entirely verbal item, which asks students to see what would happen what would happen to the sample size if they got the same effect with a larger sample.  But, that was a much harder item (45 \% correct), and the correlation between the two appears to be rather low. The more popular distractor was to say that the p-value would be the same in the larger sample size, which is what we would expect when people do not incorporate the effect of sample size on sampling variability.  But there may be a good deal of other misunderstandings built into these responses, because that would presuppose that people showed good understanding of the p-value as well.

People did better on the candies problem\ldots{} I believe the previous performance was in the 20s or 30s?  Here, 50\% got it correct.  It's a few less steps now, we directly ask to evaluate the variability of the sampling distribution, rather than asking them to reason about the likelihood of a deviation from expectation. One way to solve this is to follow through the \emph{logical consequences} of the variability, but that adds steps onto the task.  The dominant misconception is, as before, that the larger and smaller have the same variability, but something about seeing this representation may cue them to think in a different manner about the issue.

Interestingly, the dominant misconception on the confidence interval one (which was also near 50\%) was that the larger sample size would have a larger confidence interval.  We have no idea how students are thinking about this, but it may be due to the complicated nature of confidence intervals more than a conception about the empirical distribution of sample statistics. It'd be interesting to crosstab these two.


\subsubsection{{\bfseries\sffamily TODO} Compare p-value item to other p-value items\ldots{} what happens to those who get the other two \emph{right}?}
\label{sec-3-2-1}

\subsubsection{{\bfseries\sffamily TODO} In general, I could do an analysis of performance on sample size items among the highest-performing students that appear to show good reasoning skills about everything besides for sample size.}
\label{sec-3-2-2}

Or look at different slices of students at different total (or total-minus-sample-size) levels

\subsubsection{{\bfseries\sffamily TODO} crosstab/mosaic plot of q09 vs q13}
\label{sec-3-2-3}

\subsection{Focus on center}
\label{sec-3-3}
\begin{center}
\begin{tabular}{rl}
\textbf{Items} & \textbf{Gloss}\\
\hline
10 & Headache problem\\
11 & Weight training 1: center\\
\end{tabular}
\end{center}

I didn't include q07, the mean/median item, because it's not really about inference; instead, it's about what \emph{affects} the mean and median.
\begin{Schunk}
\begin{Sinput}
> cents <- c("q10", "q11")
> cents.rw <- paste0(cents, ".rw")
> summary(gp[,sort(c(cents, cents.rw))])
\end{Sinput}
\begin{Soutput}
                  q10          q10.rw                       q11     
 Standard medication: 99   Min.   :0.0000   Smaller difference:162  
 New medication     :859   1st Qu.:0.0000   Larger difference :883  
 Neither            :205   Median :1.0000   Same evidence     :115  
 NA's               :  2   Mean   :0.7386   NA's              :  5  
                           3rd Qu.:1.0000                           
                           Max.   :1.0000                           
                           NA's   :2                                
     q11.rw      
 Min.   :0.0000  
 1st Qu.:1.0000  
 Median :1.0000  
 Mean   :0.7612  
 3rd Qu.:1.0000  
 Max.   :1.0000  
 NA's   :5       
\end{Soutput}
\begin{Sinput}
> round(cor(gp[,cents.rw], use = "complete.obs"), 2)
\end{Sinput}
\begin{Soutput}
       q10.rw q11.rw
q10.rw   1.00   0.11
q11.rw   0.11   1.00
\end{Soutput}
\end{Schunk}

\subsection{Variability}
\label{sec-3-4}
\begin{center}
\begin{tabular}{rl}
\textbf{Items} & \textbf{Gloss}\\
\hline
8 & Standard deviation dotplots\\
12 & Weight training 2: variability\\
\end{tabular}
\end{center}


\begin{Schunk}
\begin{Sinput}
> vars <- c("q08", "q12")
> vars.rw <- paste0(vars, ".rw")
> summary(gp[,sort(c(vars, vars.rw))])
\end{Sinput}
\begin{Soutput}
       q08          q08.rw                        q12          q12.rw      
 Uniform :495   Min.   :0.0000   Smaller variability:759   Min.   :0.0000  
 Unimodal:429   1st Qu.:0.0000   Larger variability :235   1st Qu.:0.0000  
 Neither :240   Median :0.0000   Same evidence      :169   Median :1.0000  
 NA's    :  1   Mean   :0.4253   NA's               :  2   Mean   :0.6526  
                3rd Qu.:1.0000                             3rd Qu.:1.0000  
                Max.   :1.0000                             Max.   :1.0000  
                NA's   :1                                  NA's   :2       
\end{Soutput}
\begin{Sinput}
> round(cor(gp[,vars.rw], use = "complete.obs"), 2)
\end{Sinput}
\begin{Soutput}
       q08.rw q12.rw
q08.rw   1.00   0.07
q12.rw   0.07   1.00
\end{Soutput}
\end{Schunk}
% Emacs 24.2.1 (Org mode 8.2.1)
\end{document}
